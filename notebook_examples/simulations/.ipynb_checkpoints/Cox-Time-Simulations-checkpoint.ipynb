{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main = os.chdir(os.path.dirname(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from model import model_architecture, output_results, utils\n",
    "from pycox.models import CoxTime\n",
    "from sksurv.util import Surv as skSurv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the censoring rate of the simulated data (it can be either 0.2, 0.4 or 0.6). Data is simulated by the random function generator introduced by Friedman et al. (2001). \n",
    "Data is normalized (with mean and std from train set for train and test set) and splitted into training and test set (df_train and df_test are subsets of df_sim). The same training and test set are used for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"CoxTime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = \"0.2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_sim = \"data/simulations/\"+rate+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = pd.read_csv(dir_sim+'simdata.csv')\n",
    "df_train = pd.read_csv(dir_sim+'sim_train.csv')\n",
    "df_test = pd.read_csv(dir_sim+'sim_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = utils.normed_data(df_train, df_test)\n",
    "\n",
    "def get_target(df): return df['yy'].values, df['status'].values\n",
    "labtrans = CoxTime.label_transform(with_mean = False, with_std = False)\n",
    "y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "duration_test, event_test = get_target(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Model's construction and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the architecture are the one listed in the parameters dataframe, selected by a 5-fold cross-validation among 100 sets of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = pd.read_csv(\"model/param_simu_\"+rate+\".csv\",sep=';', index_col = 0).T\n",
    "param_final = param.loc[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neurons           128\n",
       "drop              0.6\n",
       "activation       relu\n",
       "lr_opt         0.0025\n",
       "optimizer     rmsprop\n",
       "n_layers            2\n",
       "Name: CoxTime, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = int(param_final['neurons'])\n",
    "drop = float(param_final['drop'])\n",
    "activation = param_final['activation']\n",
    "lr_opt = float(param_final['lr_opt'])\n",
    "optimizer = param_final['optimizer']\n",
    "n_layers = int(param_final['n_layers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is used to define the architecture of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 0.8181\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 0.7274\n",
      "2:\t[0s / 0s],\t\ttrain_loss: 0.6949\n",
      "3:\t[0s / 0s],\t\ttrain_loss: 0.6729\n",
      "4:\t[0s / 0s],\t\ttrain_loss: 0.6283\n",
      "5:\t[0s / 0s],\t\ttrain_loss: 0.6615\n",
      "6:\t[0s / 0s],\t\ttrain_loss: 0.6290\n",
      "7:\t[0s / 0s],\t\ttrain_loss: 0.6366\n",
      "8:\t[0s / 1s],\t\ttrain_loss: 0.6186\n",
      "9:\t[0s / 1s],\t\ttrain_loss: 0.6007\n",
      "10:\t[0s / 1s],\t\ttrain_loss: 0.6372\n",
      "11:\t[0s / 1s],\t\ttrain_loss: 0.6263\n",
      "12:\t[0s / 1s],\t\ttrain_loss: 0.6280\n",
      "13:\t[0s / 1s],\t\ttrain_loss: 0.6148\n",
      "14:\t[0s / 1s],\t\ttrain_loss: 0.6204\n",
      "15:\t[0s / 1s],\t\ttrain_loss: 0.6228\n",
      "16:\t[0s / 1s],\t\ttrain_loss: 0.6113\n",
      "17:\t[0s / 2s],\t\ttrain_loss: 0.6098\n",
      "18:\t[0s / 2s],\t\ttrain_loss: 0.6199\n",
      "19:\t[0s / 2s],\t\ttrain_loss: 0.5980\n",
      "20:\t[0s / 2s],\t\ttrain_loss: 0.6132\n",
      "21:\t[0s / 2s],\t\ttrain_loss: 0.6055\n",
      "22:\t[0s / 2s],\t\ttrain_loss: 0.6025\n",
      "23:\t[0s / 2s],\t\ttrain_loss: 0.5982\n",
      "24:\t[0s / 2s],\t\ttrain_loss: 0.5988\n",
      "25:\t[0s / 2s],\t\ttrain_loss: 0.5790\n",
      "26:\t[0s / 2s],\t\ttrain_loss: 0.6007\n",
      "27:\t[0s / 3s],\t\ttrain_loss: 0.6080\n",
      "28:\t[0s / 3s],\t\ttrain_loss: 0.5854\n",
      "29:\t[0s / 3s],\t\ttrain_loss: 0.5747\n",
      "30:\t[0s / 3s],\t\ttrain_loss: 0.6058\n",
      "31:\t[0s / 3s],\t\ttrain_loss: 0.5897\n",
      "32:\t[0s / 3s],\t\ttrain_loss: 0.5694\n",
      "33:\t[0s / 3s],\t\ttrain_loss: 0.5833\n",
      "34:\t[0s / 3s],\t\ttrain_loss: 0.5725\n",
      "35:\t[0s / 3s],\t\ttrain_loss: 0.5870\n",
      "36:\t[0s / 4s],\t\ttrain_loss: 0.5663\n",
      "37:\t[0s / 4s],\t\ttrain_loss: 0.5743\n",
      "38:\t[0s / 4s],\t\ttrain_loss: 0.5570\n",
      "39:\t[0s / 4s],\t\ttrain_loss: 0.5820\n",
      "40:\t[0s / 4s],\t\ttrain_loss: 0.5606\n",
      "41:\t[0s / 4s],\t\ttrain_loss: 0.5406\n",
      "42:\t[0s / 4s],\t\ttrain_loss: 0.5581\n",
      "43:\t[0s / 4s],\t\ttrain_loss: 0.5504\n",
      "44:\t[0s / 4s],\t\ttrain_loss: 0.5246\n",
      "45:\t[0s / 4s],\t\ttrain_loss: 0.5654\n",
      "46:\t[0s / 5s],\t\ttrain_loss: 0.5352\n",
      "47:\t[0s / 5s],\t\ttrain_loss: 0.5405\n",
      "48:\t[0s / 5s],\t\ttrain_loss: 0.5539\n",
      "49:\t[0s / 5s],\t\ttrain_loss: 0.5589\n",
      "50:\t[0s / 5s],\t\ttrain_loss: 0.5621\n",
      "51:\t[0s / 5s],\t\ttrain_loss: 0.5198\n",
      "52:\t[0s / 5s],\t\ttrain_loss: 0.5474\n",
      "53:\t[0s / 5s],\t\ttrain_loss: 0.5506\n",
      "54:\t[0s / 5s],\t\ttrain_loss: 0.5532\n",
      "55:\t[0s / 6s],\t\ttrain_loss: 0.5967\n",
      "56:\t[0s / 6s],\t\ttrain_loss: 0.5542\n",
      "57:\t[0s / 6s],\t\ttrain_loss: 0.5604\n",
      "58:\t[0s / 6s],\t\ttrain_loss: 0.5466\n",
      "59:\t[0s / 6s],\t\ttrain_loss: 0.5485\n",
      "60:\t[0s / 6s],\t\ttrain_loss: 0.5447\n",
      "61:\t[0s / 6s],\t\ttrain_loss: 0.5114\n",
      "62:\t[0s / 6s],\t\ttrain_loss: 0.5274\n",
      "63:\t[0s / 6s],\t\ttrain_loss: 0.5429\n",
      "64:\t[0s / 6s],\t\ttrain_loss: 0.5585\n",
      "65:\t[0s / 7s],\t\ttrain_loss: 0.5469\n",
      "66:\t[0s / 7s],\t\ttrain_loss: 0.5067\n",
      "67:\t[0s / 7s],\t\ttrain_loss: 0.5257\n",
      "68:\t[0s / 7s],\t\ttrain_loss: 0.5665\n",
      "69:\t[0s / 7s],\t\ttrain_loss: 0.5141\n",
      "70:\t[0s / 7s],\t\ttrain_loss: 0.5382\n",
      "71:\t[0s / 7s],\t\ttrain_loss: 0.4918\n",
      "72:\t[0s / 7s],\t\ttrain_loss: 0.5550\n",
      "73:\t[0s / 7s],\t\ttrain_loss: 0.5150\n",
      "74:\t[0s / 8s],\t\ttrain_loss: 0.5375\n",
      "75:\t[0s / 8s],\t\ttrain_loss: 0.5034\n",
      "76:\t[0s / 8s],\t\ttrain_loss: 0.5326\n",
      "77:\t[0s / 8s],\t\ttrain_loss: 0.5177\n",
      "78:\t[0s / 8s],\t\ttrain_loss: 0.5172\n",
      "79:\t[0s / 8s],\t\ttrain_loss: 0.4915\n",
      "80:\t[0s / 8s],\t\ttrain_loss: 0.5240\n",
      "81:\t[0s / 8s],\t\ttrain_loss: 0.4619\n",
      "82:\t[0s / 8s],\t\ttrain_loss: 0.4945\n",
      "83:\t[0s / 8s],\t\ttrain_loss: 0.5369\n",
      "84:\t[0s / 9s],\t\ttrain_loss: 0.5407\n",
      "85:\t[0s / 9s],\t\ttrain_loss: 0.5400\n",
      "86:\t[0s / 9s],\t\ttrain_loss: 0.5305\n",
      "87:\t[0s / 9s],\t\ttrain_loss: 0.5257\n",
      "88:\t[0s / 9s],\t\ttrain_loss: 0.5448\n",
      "89:\t[0s / 9s],\t\ttrain_loss: 0.4974\n",
      "90:\t[0s / 9s],\t\ttrain_loss: 0.5090\n",
      "91:\t[0s / 9s],\t\ttrain_loss: 0.5001\n",
      "92:\t[0s / 9s],\t\ttrain_loss: 0.5074\n",
      "93:\t[0s / 10s],\t\ttrain_loss: 0.4794\n",
      "94:\t[0s / 10s],\t\ttrain_loss: 0.4725\n",
      "95:\t[0s / 10s],\t\ttrain_loss: 0.4952\n",
      "96:\t[0s / 10s],\t\ttrain_loss: 0.5146\n",
      "97:\t[0s / 10s],\t\ttrain_loss: 0.4750\n",
      "98:\t[0s / 10s],\t\ttrain_loss: 0.4895\n",
      "99:\t[0s / 10s],\t\ttrain_loss: 0.4581\n"
     ]
    }
   ],
   "source": [
    "model,callbacks  = model_architecture.objective(x_train,  neurons, drop, activation, lr_opt, optimizer, n_layers,name, labtrans = labtrans)\n",
    "log = model.fit(x_train, y_train,neurons, epochs = 100,callbacks = callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here the results for one simulation dataset. We then simulate 100 datasets for one censoring rate and output the results for the 100 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.compute_baseline_hazards()\n",
    "surv = model.predict_surv_df(x_test)\n",
    "results_all = output_results.output_simulations(surv,df_train, x_test, df_test,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We output the median survival time, the AUC value for this time, Uno's C-index for median time and the final censoring rate of the simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_med</th>\n",
       "      <th>auc_med</th>\n",
       "      <th>unoc</th>\n",
       "      <th>cens_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.203649</td>\n",
       "      <td>0.866017</td>\n",
       "      <td>0.810271</td>\n",
       "      <td>19.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t_med   auc_med      unoc  cens_rate\n",
       "0  1.203649  0.866017  0.810271  19.333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnet_compare",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
